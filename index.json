[{"authors":["admin"],"categories":null,"content":"Welcome to my project portfolio for the Master of Science in Geographic Information Science (MSGIS) program at the University of Utah. Prior to starting the MSGIS program, I spent 10 years on active duty as an Air Force Weather Officer. My educational background is in Meteorology (B.S. from Nebraska in 2005) and Atmospheric Science (M.S. from Utah in 2014), and I was introduced to the power of GIS during my time in the weather career field.\nSince leaving the Air Force, I’ve completed GIS internships in Sarpy County’s (Nebraska) Public Works Department, the Utah Automated Geographic Reference Center (AGRC), and RedCastle Resources, Inc. In late 2018, I returned to the AGRC and currently work full-time as a GIS Specialist. I’ve enjoyed growing and expanding my GIS skills and applying them to different problems. Outside of school and work, my wife and I spend most of our time chasing around our 2-year-old daughter. I also enjoy running, hiking, snowboarding, and watching and playing most sports.\n","date":-62135596800,"expirydate":-62135596800,"kind":"term","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://eneemann.github.io/author/erik-neemann/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/author/erik-neemann/","section":"authors","summary":"Welcome to my project portfolio for the Master of Science in Geographic Information Science (MSGIS) program at the University of Utah. Prior to starting the MSGIS program, I spent 10 years on active duty as an Air Force Weather Officer.","tags":null,"title":"Erik Neemann","type":"authors"},{"authors":null,"categories":null,"content":" Poster Final Report  Introduction The invasive species Phragmites australis, or common reed, causes numerous problems and poses a significant threat to Utah wetlands, including the Great Salt Lake. The tall reed grows very densely into invaded regions, crowds out native species, disrupts migratory bird habitats, and negatively impacts recreation. It also alters the surface water regime, often converting submerged or saturated soils to dry land. As a result, the Utah Department of Natural Resources, Division of Forestry, Fire, and State Lands (FFSL), has taken an aggressive approach to mitigating the Phragmites problem. A big component of Phragmites mitigation includes spraying a wetland-safe herbicide to kill the plant and allow native species to return (Figure 1) using GPS-guided equipment. Efficient spraying requires accurate geographic data representing Phragmites boundaries, but it is currently unavailable. This project generates Phragmites maps from remotely sensed imagery, including satellite and unmanned aerial system platforms (UAS), using a prototype, automated Python script.\n  Figure 2. Reference map of Howard Slough mitigation area (left) and study area for this project, including the UAS imagery (right).   Methodology Remotely sensed imagery was collected from three different platforms to facilitate DNR’s Phragmites mitigation work in the Howard Slough Waterfowl Management Area (Figure 2). The image platforms (Table 1), include the European Space Agency’s Sentinel-2 satellite, DigitalGlobe’s WorldView-2 (WV-2) satellite, and PMG Vegetation’s UAS. Each platform collects imagery in different wavelength bands and spatial resolutions.\n  Table 1. Remote sensing imagery platforms used in this study and their attributes.   These images are then passed into an automated Python script (workflow depicted in Figure 3) that exports a shapefile for use in GPS-enabled Phragmites spraying equipment.\n  Figure 3. Diagram of automated script workflow. The left columns describe the script’s primary functions, while the right columns represent of snapshot of each function’s output.   Results The final classified images show quite different results (Figure 4), but each platform scored well (above 0.9) on several validation metrics (Table 2).\n  Table 2. A sample of accuracy metrics for each platform’s final classification. Metrics are calculated from independent validation pixels that were not used to train the model.   While classification differences are expected for a complex wetland scene, the high scores for each platform may indicate that the training/validation data samples were unambiguous and easy to classify. Sentinel-2 even had a perfect score for producer’s accuracy with the live Phragmites class. Both objective accuracy metrics and subjective assessment indicate that WorldView-2 performed the best. It appears to have the most probable and coherent classification, with the best representation of dead Phragmites in previously-treated areas. The UAS results are much less coherent, but the higher resolution may better capture small pockets of water and native emergent vegetation. All platforms appear to over-classify live Phragmites and may have difficulty in distinguishing it from other forms of live vegetation. Generally, both higher resolution data and additional wavelength bands appear to improve image classification.\n  Figure 4. Standard red, green, blue (RGB) image from each platform (top row) and final classified land cover image using object-based random forest model (bottom row).   Discussion Overall, the methods employed in this study produced reasonable land cover classification results and successfully generated a shapefile identifying Phragmites boundaries from an automated workflow. However, there were several limitations to this study that prevent strong conclusions from being drawn, including:\n Ground truth data was not available; therefore, training and validation data was derived through visual interpretation of high-resolution imagery. Asynchronous images were assumed to be collected at the same time and the same set of training/validation data was used on images from all platforms. Image resolution differences resulted in a large discrepancy between platforms in the number of pixels available to train and validate the random forest model (Figure 5; Table 3). This limits the validity of strictly using the objective metrics to assess classification performance.    Table 3. Number of training and validation pixels for each imagery platform.   With these in mind, the study could be improved if ground truth reference data was gathered with a random sampling strategy at the time image collection. This would limit temporal differences and remove subjectivity from the data collection process.\n  Figure 5. Example training/validation polygon (red outline) overlaid on RGB imagery from each platform to demonstrate the difference in the number pixels available for training and validation across platforms.   Future Work  Collect ground reference data at the time of image collection Employ a 5-band multispectral sensor to gather high-resolution UAS imagery in red, green, blue, red-edge, and near infrared wavelengths Experiment with the number of land covers used in the classification Perform additional optimizations to the image segmentation and random forest algorithms  Additional details on all aspects of this project, including references and citations can be found in the Full Report.\nMSGIS Program Skills  GIS Analysis Spatial Data and Algorithms GIS Workflow Model Building Cartography and Graphic Design Data Model and Structures Project Design Project Management Communication Skills Scripting (Python)  ","date":1556582400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1556582400,"objectID":"13f4b92cb34ac5dba5bcd874075c768d","permalink":"https://eneemann.github.io/project/capstone/","publishdate":"2019-04-30T00:00:00Z","relpermalink":"/project/capstone/","section":"project","summary":"Machine Learning Image Classification","tags":["GIS Analysis","GIS Workflow","Spatial Algorithms","Model Building","Project Design","Project Management","Communication","Data Models","Scripting","Machine Learning","Python"],"title":"Capstone","type":"project"},{"authors":null,"categories":null,"content":" Slides Final Report  Introduction Wildfire is a common hazard faced by large regions in the western US due to the combination of vegetation/fuels, climate, topography, and ignition sources. In recent decades, with increases in population, communities and residential areas have continually expanded outward from urban centers. In the mountainous west, this expansion typically leads to additional development in the urban-wildland interface, thereby increasing the number of people exposed to the potential wildfire hazard. Identifying areas of wildfire susceptibility is important to ensure at-risk populations have emergency management plans and are able to mitigate fire risk as much as possible. The most effective method of identifying risk areas is producing maps by combining multiple wildfire parameters in a geographic information system (GIS). This study aims to quantify wildfire susceptibility in Utah and identify locations at greatest risk.\nA multi-layer perceptron (MLP) neural network (NN) was chosen as the model for this study due to the recent success of other research projects in implementing NNs to predict forest fire susceptibility (Satir et al., 2015; Bui et al., 2017). Neural Networks are a natural choice for analyzing fire potential because they are good at approximating complex functions, are able to capture both linear and nonlinear relationships that exist between predictors and dependent variables, and adaptively learn from training data (Kantardzic, 2011; Satir et al., 2015).\nData All data used in this research came from free, publicly-available sources. The original data sets were initially gathered on disparate grids (different cell sizes, projections, dimensions, etc.) and were aligned to a uniform grid via projections in ESRI’s ArcMap software and with the use of ArcPy scripts. The data were also clipped down to the same geographic areas of interest (figure 1). This resulted in raster data sets with a Universal Transverse Mercator coordinate system in zone 12 North (UTM 12N). Each grid cell is 30 by 30 m with total dimensions of 2867 by 4251 pixels. The data was then resampled to a 90 by 90 m grid to alleviate computational challenges encountered on the full-resolution data.\nA snapshot of all data sets used in the NN model is shown in Figure 1, below. A description of each data set can be found in the following paragraphs.\n  Figure 1. Snapshot of the data used in the neural network.   Historical Fires\n Historical fire occurrence and burn severity data was collected from the Monitoring Trends in Burn Severity (MTBS) project. For this study, data from 2007-2016 was used to train the NN in order to place an emphasis on the most recent decade of fire data, which should best correspond with recent vegetation and infrastructure data. The ten annual data sets from 2007-2016 where then reclassified to only include burned areas (data value of 1-4), and summed up into a single data set. This resultant MTBS fire data set accounts for a combination of cumulative fire occurrences (multiple fires) and burn severities, called the historical fire index.  Terrain\n The Digital Elevation Model (DEM) used in this study was downloaded from the Utah Wildfire Risk Assessment Portal (WRAP). Slope and aspect data sets were calculated from the DEM and used as inputs into the NN model. These three terrain-related data sets are important for the neural network model because each has an influence on wildfire and fire-related parameters.  Vegetation\n Vegetation data sets were also gathered from the Utah WRAP, each on the same UTM 12N, 30 m grid as the DEM data. The vegetation type data set categorizes each grid cell into one of 19 different categories. The vegetation type information is important for identifying the type of biomass available for burning, where some vegetation types burn more readily than others due to their biophysical properties (Bui et al., 2017). Two additional vegetation data sets, canopy cover (%) and bulk canopy density (kg/m3 * 100) are also included to help quantify the amount of biomass fuel that is available for burning on a grid cell.  Infrastructure\n A statewide Utah roads data set was gathered from the Utah Automated Geographic Reference Center (AGRC). The vector road features were used to calculate a distance-to-roads raster layer using Euclidean distance, meaning the minimum distance from a pixel to a road is calculated “as the crow flies.” Distance to roads is employed as a mechanism to measure the anthropogenic influence on wildfires and relates to the potential cause of wildfires (Satir et al., 2015; Bui et al., 2017).  Climate\n Climate data for this study was collected from Oregon State’s PRISM Climate Group. Monthly 30-year normal temperature and precipitation data sets were downloaded with 800 m grid cells. Only data from the months of June through October were used, representing the primary fire season in Utah. The monthly climate data sets, for both temperature and precipitation, were averaged into a single data set representing the mean conditions for the entire fire season. These data were resampled to 30 m grid cells with cubic interpolation.  Methods Data Preprocessing\nIn order to prepare the data for input into the neural network model, several preprocessing steps were necessary. First, the data was scaled with a max-min normalization to place each data set between a range of 0 to 1. This was needed to ensure that the scale variations among the different data sets didn’t lead one or two variables to overwhelm the training process and contaminate the results.\nThe historical fire severity MTBS data, used for training the NN, was thinned in order to improve the training process. The vast majority of the area of interest is comprised of pixels that have not been burned (~98%), but it was desirable to have better balance among burned and unburned pixels in the training data set. For this reason, a subset of the entire AOI was used for training, where polygons around the burned areas where chosen to provide a more balanced mix of burned/unburned pixels (black polygons in figure 2). This also improved the efficiency of training the NN, as data set used for training was about 10% of the original size.\n  Figure 2. Historical fire severity data used to train neural network.   Artificial Neural Network\nThe R statistical language was used to implement the NN using RStudio software, specifically with the ‘neuralnet’ library. A forward-feed MLP NN was employed with a resilient backpropagation learning algorithm. The network was “fully-connected,” meaning that every neuron in a layer was connected to every neuron in adjacent layers. The final NN architecture including 15 hidden layers and 1 output node, representing fire susceptibility. In addition to building a NN model with the full complement of input data sets, a second model was generated without the vegetation type data. This was done to examine the influence of vegetation type and its effect on the results. Other than the removal of the vegetation type categories, there were no other differences between the models. This model will hereafter be referred to a “NVT.”\n  Table 1. Final categorization of fire susceptibility model.   Finally, the output of each model was normalized with a min-max normalization method to put the Fire Susceptibility Index on a 0-1 scale. The results from each model were then categorized into 6 fire susceptibility groups with a natural breaks methodology. However, because the categories from the Original and NVT models were very similar, the final categorization for each model was standardized based on table 1. This created a consistent symbology for generating fire susceptibility maps and allowed for a more direct comparison between the two models. A diagram showing the final architecture of the NVT model is shown in figure 3.\n  Figure 3. Final neural network architecture and weights for the No Vegetation Type (NVT) model.   Results The output from each of the NN models showed results with many similarities and some notable differences. Both models had a strong histogram peak centered on 0.55 (not shown), but the distribution of values around that peak varies. In the Original model, the vast majority of pixels were placed between 0.4 and 0.8, with a narrower distribution and shorter tails. The NVT model, however, had a wider distribution and longer tails, with a generally broader range between 0.2 and 0.85. This indicates that the Original model has many more values scoring in the middle of the range, while the NVT model’s larger range has more pixels assessed with “very low” or “extreme” values. Visually, this is depicted in figure 4, which displays a map of the Wasatch Front region with the Fire Susceptibility Index calculated for each pixel. Compared to the Original Model, the NVT model has more values in the “extreme” category for high-threat areas such as Antelope Island in the Great Salt Lake, the northern Oquirrh Mountains, the Lake Mountains, and the mountain range south of Utah Lake. The NVT model also has a larger quantity of “very low” pixels along the high peaks of the Wasatch Front and Oquirrh Mountains.\n  Figure 4. Fire Susceptibility Index from the Original and NVT models with census tracts outlined in black.   Another notable difference between the models is the apparent influence of the vegetation type variable in the Original model and its lack of influence in the NVT model (where it was removed). The Original model has a generally noisier appearance, reflective of the noisy nature of the vegetation type variable, whereas the NVT model has smoother, more gradual transitions from low to high index values. The Original model also highlights the area between Salt Lake International Airport and the Great Salt Lake with “very high” and “extreme” index values, while the NVT model shows “very low” to “moderate” values. Here, the Original model appears to be picking up on increased fire susceptibility from the shrubland and chaparral vegetation types. The NVT model also tends to be more strongly influenced by elevation and aspect in mountainous regions, whereas the Original model is strongly influenced by vegetation type.\nThe influence of vegetation type is further examined by plotting variable importance based on NN connection weights from each model, as seen in figure 5. Indeed, the 10 most important variables in the Original model are vegetation types, which also make up 15 of the 18 most important variables. The top non-vegetation variables include elevation, precipitation, and distance to roads. For the NVT model, the top 3 variables are precipitation, elevation, and distance to roads. This comparison shows the importance of the vegetation type variables in the Original model, perhaps even indicating that it is over-influencing results.\n  Figure 5. Variable importance plots for Original and NVT models.   A final, quantitative comparison of the Original and NVT models is presented in table 2. Mean squared error (MSE) and mean absolute error (MAE) metrics are calculated for each model using the subset of validation data from training the NN models. These metrics both indicate that the NVT model performs better as its errors are roughly half of the Original model’s error. The area under the curve (AUC) metric for the receiver operating characteristic (ROC) curve was also calculated for each model. This value accounts for a balance between true positive rate and false positive rate when using a broad range of thresholds to classify model results. The AUC metric indicates that the Original model (0.8875) outperformed the NVT model (0.8127) as the Original model had a higher value and was closer to the perfect score of one. These two sets of metrics (errors and AUC) show conflicting results without a clear winner on which model performs best. It should also be noted that these metrics only consider about 3% of all pixels in the study area, those used for validation of the training data.\n  Table 2. Quantitative results comparing Original and NVT models. Mean Squared Error, Mean Absolute Error, and Receiver Operating Characteristic Area Under the Curve calculated from the validation data subset.   Conclusions Overall, this study demonstrates that the NN methodology produced reasonable results in quantifying wildfire susceptibility along Utah’s Wasatch Front. Particularly, it shows that vegetation type may play a prominent role in estimating wildfire susceptibility, as was noted by the model differences between the Original an NVT models. The inclusion of vegetation type often produced noisier results, reflecting the noisy nature of the vegetation type data itself. It also demonstrated local maxima that appeared to be directly tied to the presence of specific vegetation types (e.g., grassland, exotic herb, and chaparral). A comparison of variable importance from the Original and NVT models showed that vegetation type dominated the top 18 variables in the Original model (Figure X). This reliance on vegetation type may have actually been to the detriment of the model and other methods of employing vegetation information may improve results. The removal of vegetation type in the NVT model allowed other variables to dominate the model results, often varying by geographic location (e.g., elevation, aspect).\nAreas with low fire susceptibility were often characterized by the presence of water, location near urban centers, or at very high elevations. Higher fire susceptibility regions tended to exist at moderate elevations and included the presence of specific vegetation types (e.g., grassland, chaparral) in the Original model. The NVT model tended to have a larger range in fire susceptibility values, with the greatest number of pixels in the “very low” and “extreme” categories.\nAdditional details on all aspects of this project, along with references and citations can be found in the Full Report.\nMSGIS Program Skills  GIS Analysis Spatial Data and Algorithms GIS Workflow Model Building Cartography and Graphic Design Spatial Analysis Data Model and Structures Database Design Project Design Project Management Communication Skills Scripting (R and Python)  ","date":1544313600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1544313600,"objectID":"a08b21c138c01680ba0d766793124a8b","permalink":"https://eneemann.github.io/project/geocomputation/","publishdate":"2018-12-09T00:00:00Z","relpermalink":"/project/geocomputation/","section":"project","summary":"Machine Learning Wildfire Analysis","tags":["GIS Analysis","GIS Workflow","Spatial Algorithms","Model Building","Project Design","Project Management","Communication","Cartography","Spatial Analysis","Data Models","Database Design","Scripting","Machine Learning","R"],"title":"Geocomputation","type":"project"},{"authors":null,"categories":null,"content":" Slides Script Output Example  Objective The objective of this project was to create a Python script tool that answers the question: “Who Taxes Me?” for residents of Utah. To accomplish this, a Python script was written to accept an input address from a user (and optional property value) and provide a simple map of the location/property, a listing of entities that levy taxes on the property, and the levy rate and amount. The listing includes the tax levy as a percentage or, optionally, as a dollar amount if a property value is provided. The data is also presented in bar graph form to visually represent which entities levy higher taxes. Finally, the script produces an automated map of the property location and tax levy information in PDF format.\n  Image source: https://www.davegranlund.com/cartoons/2005/01/23/property-tax-bills/   Data and Methods Data for this study was pulled from the Utah Automated Geographic Reference Center (AGRC) web page, which has tax entity datasets available as shapefiles and geodatabase feature classes. The study area incorporates the entire state of Utah and the script works for any address located within the state. The Python code utilizes several different data structures and methods for completing the task, primarily employing the ArcPy module. Data structures include lists, dictionaries, and Pandas dataframes, while the spatial data format is an ESRI geodatabase and feature classes. An outline of the Python script workflow is outline in Figure 1.\n  Figure 1. Diagram of Python script workflow   Results The final results showing the script tool (Figure 2) a code snippet (Figure 3), and the exported PDF document (Figure 4) are shown below:\n  Figure 2. Script tool interface and typical processing dialog window with execution time.     Figure 3. Example snippet of Python code.     Figure 4. Example PDF exported by the Python tool.   Python Libraries  ArcPy Numpy Pandas Matplotlib  MSGIS Program Skills  GIS Analysis Spatial Data and Algorithms GIS Workflow Model Building Communication Skills Scripting (Python)  ","date":1543795200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1543795200,"objectID":"38573f13dfa54337bdfc359126f87307","permalink":"https://eneemann.github.io/project/geoprocessing-with-python/","publishdate":"2018-12-03T00:00:00Z","relpermalink":"/project/geoprocessing-with-python/","section":"project","summary":"Python Scripting","tags":["GIS Analysis","Spatial Algorithms","GIS Workflow","Model Building","Communication","Scripting","Python"],"title":"Geoprocessing with Python","type":"project"},{"authors":null,"categories":null,"content":" Full Report Model Description Document NetLogo Code  Background In recent decades, the population in the US has grown outward from urban centers and begun to encroach on the urban-wildland interface, putting developments and communities in jeopardy of being affected by wildfires. This has become more common occurrence and will continue to be as populations increase along with extreme precipitation and drought events. To help analyze this problem we hope to build a model to simulate wildfire spread and community evacuation within a wildland-urban setting. This type of model could be useful when attempting to make planning decisions and informing populations of risk when developing within these areas.\nEagle Mountain, Utah is relatively new area of development with rapid growth along the urban-wildlife interface. This growth puts some locations at risk of wildfire due to their proximity to vulnerable vegetation in the nearby hills. This research seeks to model the potential impacts of wildfire on the surrounding area and to recommend some evacuation strategies to local residents. An approach for modeling wildfire dispersion patterns can be done with a percolation model. The percolation model will combine vegetation, elevation and fire threat layers to observe the probability of wildfire occurrence and its characteristics within the study area. Agent based modeling can be used to find out what evacuation strategies will work best under the local context. It can be assumed that each household will possess one vehicle (agent) and the optimal routing options (ruleset) to evacuate the total number of vehicles will be designated through this study. Implementing this model will help to observe how wildfire from varying points of origin will produce different routing patterns.\nData and Methods The model designed in this project simulates wildfire ignition and spread in the vicinity of Eagle Mountain, Utah and subsequent evacuation using a simplified road network. To test the model and learn how changes in parameters impact the simulation outcome, a suite of experiments were run using the NetLogo BehaviorSpace tool. Datasets used within the model include a Fire Threat raster from Utah’s Department of Natural Resources (DNR), which is used to determine where the initial fire starts. Vegetation data from the DNR is used to set the vegetation density, which affects the probability that patches neighboring a currently burning patch catch fire. A digital elevation model from DNR is also used to modify the probability of fire spread. Finally, a road network from the city of Eagle Mountain was simplified and used as the road network for vehicles evacuating neighborhoods.\n  Table 1. Parameter values used in model setup. Bracketed values indicate a range, notated in the following manner: [low-value step high-value].   The model has adjustable parameters for the probability of fire ignition in each cardinal direction (e.g. “ignitionS”), the probability of fire extinction (“extinction”) at each time step, and the probability that exiting cars will correctly follow their evacuation route when they approach an intersection (“q”). To experiment with the model, two primary experiments were designed, one representing a fire in southwesterly winds and one representing a fire in northwesterly winds. These two scenarios represent the most likely directions for strong winds during a wildfire event. This is particularly true for the southwesterly scenario, which typically occurs in a dry environment ahead of approaching low pressure systems. These experiments are run over a variety of settings for “extinction” and “q”, with each permutation run for 25 iterations. Table 1 describes the model setup; brackets indicate a range of values in the following manner: [low-value step high-value]. An example screenshot of a typical model run in mid-simulation is shown in Figure 1.\n  Figure 1. Example screenshot of typical model run.   Results The outcomes from the two model scenarios showed several similar results, but also indicated a few subtle differences. Based on the combination of parameters used in the experiment, the most frequent model result burned less than 2.5% of the landscape (34% of simulations) and finished with 0 burned cars (45.3%; Figure 2). In fact, in 25.2% of simulations, the fire did not cross the neighborhood buffer and no evacuation was initiated. As expected, the percentage of the landscape and the number of cars burned decreased as the fire extinction rate increased (Figure 3). Both of these statistics were maximized when the fire extinction rate was zero. Interestingly, the southwesterly wind scenario (2.87) resulted in an average of 1 more car being burned per simulation than the northwesterly wind scenario (1.82). Figure 3 also indicates a higher median number of cars burned in the southwesterly scenario for each extinction value below 0.4.\n  Figure 2. Histogram of percent landscape and cars burned by scenario.     Figure 3. Box plots of percent landscape and number of cars burned by varying levels of extinction in southwesterly and northwesterly wind scenarios.   Of the simulations resulting in evacuation, there was a weak relationship between the number of cars burned and “q” (Figure 4), but it is not likely significant. This subtle trend indicates that fewer cars burn at higher “q” due to more efficient evacuation, while more cars burn at lower “q” due to more wrong turns and a longer evacuation process. This is also reflected in the duration of the simulation, where wrong turns (lower “q”) lead to slower evacuation and a longer median simulation (Figure 4).\n  Figure 4. Box plots of number of cars burned and simulation duration by varying levels of “q” in southwesterly and northwesterly wind scenarios. The “q” variable represents the probability that a vehicle makes the correct decision at an intersection to evacuate as efficiently as possible.   Finally, Figure 5 shows plots (only for simulations with evacuation) of average percentage of landscape burned, cars burned, and simulation duration as a function of both extinction and “q”. For both wind scenarios, the maximum percentage of the landscape burned occurs when “q” is minimized and the extinction rate is zero. This is expected as low “q” values prolong the simulation and the fires never actually burn out when the extinction rate is zero. The southwesterly scenario also has a greater maximum percentage burned. This is likely because the fire is designed to start near the western and southern edges of the model domain, leaving more land downwind of the fire origin when there is a southerly component to the wind. A northerly wind component tends to push fires starting near the southern border out of the model domain. Intuitively, the maximum number of cars burned occurs at the minimums of extinction rate and “q.” This combination leads to more burned cars due to widespread fires and the slow evacuation from cars making wrong turns at intersections, as discussed previously. For the simulation length, there appears to a “sweet spot” of maximum duration where low, but non-zero extinction rates lead to a slow burning fire and low “q” values result in slower evacuations. Duration quickly drops off at higher extinction rates (0.4+) where the fires burn out quickly. The northwesterly wind scenario also had a higher maximum average simulation length than the southwesterly scenario, though it’s not entire clear why. Perhaps the low extinction rates allow fires starting on the southern periphery of the domain to burn back northward against the wind, resulting in slow-but-steady progression and long simulations when “q” is low.\n  Figure 5. Percentage of landscape burned (top), cars burned (middle) and simulation duration (bottom) as a function of both extinction and “q”.   Conclusions While this model simulation is far from realistic, it does represent some of the basic processes in the real world. These include fire spread due to properties of vegetation, wind, and elevation, and first draft vehicle evacuation logic. Recommended future work on the model would include better scaling of fire/vehicle speeds, more robust traffic logic, and evacuations by specific neighborhood. Though relatively simple, the current model is still capable of reproducing some realistic results. This includes larger, longer-burning fires when the extinction probability is low or zero. It also demonstrates that vehicles taking wrong turns while evacuating the neighborhood will take longer to exit and, therefore, be more likely to be burned in the fire. Finally, it indicates that Eagle Mountain may be at greater risk from a fire in southwesterly winds, which is likely to burn a larger area and endanger more vehicles, than a fire in northwesterly winds.\nAdditional details on the methodology and results, and all references can be found in the Full Report.\nMSGIS Program Skills  GIS Analysis Spatial Data and Algorithms GIS Workflow Model Building Data Model and Structures Project Design Project Management Communication Skills Scripting (NetLogo)  ","date":1525305600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1525305600,"objectID":"2921187cbd04fa6e2f3a06c8c0919f06","permalink":"https://eneemann.github.io/project/spatial-modeling/","publishdate":"2018-05-03T00:00:00Z","relpermalink":"/project/spatial-modeling/","section":"project","summary":"Simple Wildfire Modeling","tags":["GIS Analysis","Spatial Algorithms","GIS Workflow","Model Building","Data Models","Project Design","Project Management","Communication","Scripting","R"],"title":"Spatial Modeling","type":"project"},{"authors":null,"categories":null,"content":" Slides Final Report Conference Poster  Background The Thomas fire burned areas of Ventura and Santa Barbara counties in southern California between ignition on 4 December 2017 and containment on 12 January 2018. It erupted in early December as high pressure over the Great Basin fueled strong Santa Ana winds and dry conditions. When it was finally contained in mid-January 2018, the Thomas Fire had burned 281,893 acres, making it the largest California wildfire in modern history. The fire caused the evacuation of more than 100,000 people and destroyed over 1300 structures (Cal Fire 2018). The large swath of burn-scarred land immediately became vulnerable to mudslides and debris-flows. On 9 January 2018, an atmospheric river event brought tremendous precipitation to southern California and 13.7 mm (0.54 in) of rain fell in just 5 minutes on Montecito and the nearby mountains to the north (National Weather Service 2018). The resulting debris-flows killed 21 people, destroyed 166 structures (Figure 1), and damaged 395 additional structures (Cal Fire 2018). Much of the region ended up with 7-15 cm (2.75-5.9 in) of rain in just 2 days. The combination of recent wildfires, intense precipitation, and terrain characteristics (slope severity and soil properties) generated the devastating debris-flows.\n  Figure 1. Photos from Santa Barbara County Fire-Public Information Officer Twitter: @EliasonMike, SBCFireInfo   Data All data used in this project came from free, publicly-available sources. Satellite imagery from the Sentinel-2 Multispectral Imager (MSI) was gathered from the European Space Agency via Google Earth Engine. The MSI has 13 bands spanning the visible to shortwave infrared wavelengths at variable resolutions of 10, 20, and 60m. Digital Elevation Model (DEM) data was collected from the USGS National Map at 1/9 arc-second resolution (~10m). Soil erodibility data came from the Natural Resources Conservation Service State Soil Geographic Data Base. Precipitation observations were gathered from the National Weather Service and precipitation forecasts from the operational High-Resolution Rapid Refresh (HRRR) model were downloaded from the HRRR Archive at the University of Utah. The fire perimeter (Figure 2) and watershed outlets (pour points) were derived from USGS data. The debris-flow polygons were pulled from a web map produced by the Santa Barbara Independent.\n  Figure 2. Thomas Fire perimeter and study area   Methods Sentinel-2 imagery of southern California was collected for pre-fire (3 December 2017) and post-fire (22 January 2018) conditions. The NBR (Brewer 2005) was then calculated for each image using, Band 12 (SWIR, 2190 nm) and Band 8A (NIR, 865 nm) from the formula below:\n$$NBR = 1000*\\frac{NIR - SWIR}{NIR + SWIR}$$\nA differenced NBR (dNBR) was generated from the pre- and post-fire NBR data to identify burn severity resulting from the Thomas Fire (Figure 3). The severity thresholds for moderate (\u0026gt; 350) and high (\u0026gt; 650) burns were set as the average respective threshold from California fires in 2015, according to data from Monitoring Trends in Burn Severity (MTBS). This number was then rounded up to the next multiple of 50, to be conservative. The DEM data was used in a hydrology workflow to delineate primary streams and watershed boundaries based on USGS pour points that were snapped to the manually-created streams. Slope data were also built from the 10 m DEM data set. Once the foundational data was prepared, debris-flow probabilities for the 9 January 2018 Montecito event were calculated based on the logistic regression approach outlined by Staley et al, 2016, where statistical likelihood of debris-flow occurrence, P, is:\n$$P = \\frac{e^\\chi}{1 + e^\\chi}$$\nFurther, χ is determined by the link function:\n$$\\chi = \\beta + C_{1}TR + C_{2}FR + C_{3}SR$$\nThe parameters in the link function are represented by:\n  The logistic regression approach was used to model predicted debris-flow probability based on a common USGS rainfall threshold (6 mm in 15 min), actual observations in Montecito (13.7 mm in 15 min), and rainfall rates from HRRR hourly accumulated precipitation forecasts (~1.0-8.7 mm in 15 min) valid within 2 hours of the observed debris-flows. The 0500 UTC run of the operational HRRR model was used, with hourly rainfall at the end of forecast hour 08, valid 1300 UTC. This output was selected because it was representative of observed rainfall, though the timing was off by 75-90 min from the actual rainfall maximum (~1130 UTC).\n  Figure 3. Methodology of debris flow probability calculation   Results The logistic regression approach successfully identified the debris-flow threat posed by the Thomas Fire to Montecito for the 9 January 2018 storm (Figure 4). All models indicated an elevated threat (P \u0026gt;= 0.5) for the watersheds above Montecito. When observed rainfall totals from Montecito on 9 January (13.7 mm in 15 min) were used in the model, it indicated P \u0026gt; 0.99 (not shown). The uniform rainfall threshold of 6 mm in 15 minutes, also identified the high threat, generally 0.591 \u0026lt; P \u0026lt; 0.798. Additionally, forecasts from the HRRR model demonstrated intense rainfall leading to 0.663 \u0026lt; P \u0026lt; 0.882 in the watersheds above Montecito. Predictions based on HRRR data also generated better spatial results, with the eastern portions of the Thomas Fire watersheds showing low debris-flow probability.\n  Figure 4. Debris-flow probabilities based on uniform rainfall (top-left), HRRR forecast rainfall (top-right), and probability difference (bottom)   Overall, the spatially-varying debris-flow probabilities generated with HRRR model data decreased the percentage of high-probability watersheds (see table below). When compared to the uniform precipitation probabilities, the added information from the HRRR model forecasts appear useful in isolating the greatest debris-flow threat, as opposed to generalizing high probabilities over a large area. If extended to other events, this could help reduce false alarms, constrain evacuation regions, modify response plans, and stage resources for recovery efforts.\n  Closer inspection of the Montecito area, shows the HRRR-based probabilities accurately characterized the debris-flow hazard in the watersheds above the town (Figure 5), which feed the primary streams. All 561 structures that were damaged or destroyed in the event were located in the purple damage report area of the figure below.\n  Figure 5. Zoomed-in view of watersheds in Montecito region with damage report areas   Conclusions  Calculating dNBR from Sentinel-2 imagery is an effective way to quantify burn severity and coverage. The debris-flow probability method used by the USGS and introduced by Staley et al. 2016, performed well in assessing the debris-flow threat in Montecito, CA on 9 January 2018. The use of spatially-varying precipitation forecasts from high-resolution weather models may be useful in identifying the areas of greatest debris-flow risk. This may be particularly true near large fires or fire complexes and additional research using ensemble models is recommended. While regional modification would likely be required, this methodology could potentially be extended to other locations around the world to identify regions and populations at risk from debris-flows.  Additional details on the methodology and results, including supervised classification and NBR change detection, and all references can be found in the Full Report.\nMSGIS Program Skills  GIS Analysis Spatial Data and Algorithms GIS Workflow Model Building Cartography and Graphic Design Spatial Analysis Project Design Project Management Communication Skills  ","date":1524009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524009600,"objectID":"14b940eb490a0283cbb4a396cf0aa67d","permalink":"https://eneemann.github.io/project/remote-sensing-analysis/","publishdate":"2018-04-18T00:00:00Z","relpermalink":"/project/remote-sensing-analysis/","section":"project","summary":"Wildfire and Debris-Flow Analysis","tags":["GIS Analysis","GIS Workflow","Spatial Algorithms","Model Building","Project Design","Project Management","Communication","Cartography","Spatial Analysis"],"title":"Remote Sensing Analysis","type":"project"},{"authors":null,"categories":null,"content":" Slides  Overview Note: the servers that hosted the web page for this project were retired in 2018, so a live version of the page no longer exists\nAs a result of changes in climate and population growth, there has been increased competition for freshwater in the Western U.S. in recent decades. This trend is likely to intensity in the future as populations continue to grow and uncertainty surrounding the climate in the Intermountain West continues. The objective of this project was to investigate county-level water consumption in Montana through the creation of a web map (Figure 1). Specifically, surface water usage and sources were mapped and explored.\n  Figure 1. Screenshot of final web page.   Data \u0026amp; Methods The primary data sets for this project include:\n Excel spreadsheet with county-level water consumption statistics from 2010 Shapefile of Montana county boundaries (polygons) Shapefile of Montana rivers and streams (polylines) Shapefile of Montana lakes (polygons) Shapefile of Montana cities (points)  First, to prepare the data for consumption by the web map, the spreadsheet was joined to the counties shapefile to link surface water statistics to their geographic region. Second, all shapefiles were loaded into an ArcMap document, labelled, and symbolized. Third, the data was published as a map service, which was later consumed by the web page using the ArcGIS JavaScript API v4.3. Additional organization of the web page and customization of features was completed through a combination of HTML, CSS, and JavaScript coding (Figure 2).\n  Figure 2. Snapshot of JavaScript code.   Functionality The finished web page had the following functionality:\n Query information with a pop-up window on click (Figure 3)  County Name Population Area Surface Freshwater Withdrawals Total Water Withdrawals   Layer list toggle on/off capability Basemap toggle between “streets” and “satellite” Legend display Zoom in/out by scroll or widget    Figure 3. Example of pop-up functionality.   Data Sources  Montana State Library Geographic Information Clearinghouse http://geoinfo.msl.mt.gov/  MSGIS Program Skills  Spatial Data and Algorithms Cartography and Graphic Design Communication Skills Programming and Scripting (HTML, CSS, JavaScript)  ","date":1524009600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1524009600,"objectID":"f40f70e6d3fac6d9de5f6fbfd4ca7124","permalink":"https://eneemann.github.io/project/web-gis/","publishdate":"2018-04-18T00:00:00Z","relpermalink":"/project/web-gis/","section":"project","summary":"Web Mapping","tags":["Spatial Algorithms","Cartography","Communication","Scripting","JavaScript"],"title":"Web GIS","type":"project"},{"authors":null,"categories":null,"content":" Final Report  Introduction As Uber and other rideshare platforms continue to grow in popularity, the companies amass tremendous amounts of data from the thousands of rides logged every day in major cities (Dogtiev 2017). This data would be extremely useful for municipality planners and engineers, as well as Uber drivers alike. However, Uber understands the power of this and keeps a close hold on the raw data. This study takes a more detailed look at Uber data that was manually-collected in Salt Lake City from August to November of 2017. The goal is to help answer questions, such as:\n Where should drivers position themselves to get the highest-earning rides? How accurately can a trip’s earnings be predicted based on a few variables? Do trip distances, tips, or earnings differ by rider gender?  A variety of data analysis methods were employed to interrogate the data, including: summary statistics, linear regression, generalized linear models (GLMs), regression trees and random forests, principal component analysis (PCA), geostatistics, and spatial point patterns. These techniques have been applied in a variety of ways throughout the scientific literature over the past few decades and were useful in examining the Uber data.\nData and Methods Data was collected from 125 Uber trips between August and November 2017, primarily on Friday mornings between 5:30-9:30am. Of these 125 trips, the analysis data set was trimmed down to those trips starting in Salt Lake County (117), while the remaining trips (7) were used for model verification. The spatial data were collected with a smartphone GPS application and the attributes were gathered from the Uber app and website. These attributes include trip date, start/end time, duration, distance, driver earning total, surge multiplier, tip, rider gender, latitude, longitude, and an airport trip identifier. Some analyses were conducted on the trip’s total earnings, while other methods used a corrected (standardized) value with the surge multiplier and tips removed. This was done in an effort to remove the seemingly random components of the total earnings. Polyline features from the GPS traces were collected, but the geographical analysis presented here is limited to the trip starting points (and ending points) and their associated attributes (Figure 1).\nOf the data collected, a typical trip lasted around 12.5 minutes, covering 4-5 miles with no tip or surge multiplier, resulting in a total driver earning of $5-6. While this trip is considered “typical,” there are a wide variety of trip total earnings, corrected earnings (tip \u0026amp; surge multiplier removed), distances, durations, and tips, as shown in the histograms below (Figure 2). Trip earnings and tips tend to follow an exponential distribution, while distance and duration are closer to normal or gamma distributions. The differences in these distributions created challenges in analyzing the data, resulting in some caveats for interpretation.\n  Figure 2. Histograms of total earning (left), distance (center), and duration (right) for all Uber trips.   While only 16.2% of Uber trips end at the airport, airport trips result in higher driver earnings (Figure 3); the mean airport trip earns $11.09, compared to $5.83 for non-airport trips. A T-test confirms that this is a statistically significant result at greater than 99.9% confidence. Additionally, males make up 53.8% of Uber riders, but differences in driver earnings based on rider gender are not statistically significant. Driver total earnings most closely correlated with trip distance, and Figures 4 shows a scatterplot of corrected earnings by distance. These plots also break out gender by color with males (females) in blue (pink), and airport trips by size where non-airport trips (airport trips) are small dots (large dots).\n  Figure 3. Box and whiskers plots of non-airport and airport Uber trips.     Figure 4. Scatterplot of corrected Uber trip earnings by distance, with male/female rider, and airport/not-airport trips symbolized distinctly.   Several methods that were used to analyze the Uber data will be described in the following paragraphs. First, a handful of prediction models will be discussed, followed by geostatistical and spatial point pattern analysis. The first two prediction models were built with linear regression. One was a very simple model that relied purely on distance to predict total earnings \u0026ndash; distance is known to be the largest component of Uber’s actual driver earning calculation. The second model used stepwise automatic selection starting from a null model and with a total scope of 8 variables (distance, duration, surge, tip, gender, airport, longitude, latitude). This resulted in an “optimum” model with 5 variables (distance, duration, tip, surge, gender). The third model was a GLM that used a Poisson distribution and log link function. The fourth and fifth models used regression trees, the fourth of which used a single tree, pruned to a moderately aggressive level of complexity. The fifth model employed a random forest of 500 trees, with 5 variables considered at each split. Finally, principal component regression was used for the sixth model. This applied 4 components resulting from the PCA that accounted for 98.4% of the variance in the data. Once all models were built, they were used to predict total earnings and log-earnings from the 7 data points that fell outside of Salt Lake County.\nThe next set of analyses consisted of geostatistical prediction and simulation. Ordinary kriging was initially done to estimate total earnings across Salt Lake City based on trip starting points. This was followed by conditional kriging simulations done to estimate the probability of earnings above a certain threshold. Both start and end points were used for kriging simulations, but the end point data proved much more difficult to fit with variograms, resulting in limited success.\nResults The prediction model results showed a variety of performance skill in estimating the total earnings of the 7 Uber trips that fell outside of Salt Lake County. Using root mean squared errors (RMSE) as the primary metric, the optimized linear model performed the best, followed by the PCR model and random forest model (not shown). All models showed improved performance for log-earnings (vs. total earnings; not shown), except for the PCR model (Figure 5). In this case, the random forest model was the best, followed by the single regression tree and the optimum linear model. The fact that nearly all models performed better with log-earnings, demonstrates that using log-earnings as the dependent variable is probably the best choice for regression modeling, given the distribution mentioned previously.\n  Figure 5. Scatterplot of predicted Uber trip total earning by model (left) and RMSE bar charts by model (right).   A plot of total earnings based on start points were used to interpolate a surface of values across the Salt Lake Valley with an ordinary kriging technique (not shown). The ordinary kriging results showed relatively high cross-validation errors (not shown) even though the residuals appeared random. A kriging simulation of 50 iterations was conducted across the region to predict the probability of total earnings greater than $8 (Figure 6). The simulation identified similar regions as the ordinary kriging, with the addition of a fourth high-probability region between downtown and the airport. The highest probabilities, approaching 80%, are shown near Kearns and on the east side north of I-80. Total earnings for the Uber end points (Figure 6) were also used for kriging simulations. Figure 22 shows the results, with the airport identified by high probabilities of earnings greater than $8 at 90%. Another, broad region of high probabilities covers the southeast quadrant of the Salt Lake Valley, where only a few data points exist. However, the small sample size and poor variogram fit (not shown) for the end point data make this result highly suspect. The area between downtown Salt Lake City and the university also stands out as trips ending there have very low probabilities (less than 10%) of exceeding $8.\n  Figure 6. Kriging simulations showing the probability of Uber trip earnings greater than $8 based on trip start points (left) and end points (right).   All references and citations can be found in the Full Report.\nMSGIS Program Skills  GIS Analysis Spatial Data and Algorithms GIS Workflow Spatial Analysis Data Models and Structures Database Design Communication Skills Scripting (R)  ","date":1513123200,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1513123200,"objectID":"7e4695296decc4ddaa60535840c14c1b","permalink":"https://eneemann.github.io/project/advanced-geographic-data-analysis/","publishdate":"2017-12-13T00:00:00Z","relpermalink":"/project/advanced-geographic-data-analysis/","section":"project","summary":"Uber Trip Analysis","tags":["Spatial Analysis","GIS Analysis","GIS Workflow","Spatial Algorithms","Data Models","Database Design","Communication","Scripting","Machine Learning","R"],"title":"Advanced Geographic Data Analysis","type":"project"},{"authors":null,"categories":null,"content":" Slides Final Report  Introduction This project presents a statewide recreation database for Utah. The database gathers many recreational datasets in one place for Utahns to explore and use to plan trips or activities. A variety of data related to hiking, camping, skiing, canyoneering, swimming/boating, golfing, and parks/wilderness are included. The data are associated through relationship classes (where appropriate), to help query and make sense of inherent connections. Some example queries the database helps to answer include:\n What trailheads provide access to Goblin Valley’s Dark Side of the Moon trail? How many ski lifts are in the ski resort containing “Chips Run?” What boat ramps provide access to Lake Powell?  With a wide variety of functionality, users are able to use the database to find the necessary trailhead for their desired hiking route (Figure 1) or determine what runs and lifts are available to them at a ski resort. Users can also determine what boat ramps provide access to a specific lake or river, identify which campgrounds in Washington County have more than 50 campsites, and see what Utah peaks are higher than 13,000 feet above sea level. This represents just a small sampling of information that the database can provide.\n  Figure 1. Snapshot of layers in the Utah Recreation Database   Conceptual Model A conceptual Entity-Relationship diagram was created based on the integrity constraints for the project (Figure 2). In this diagram, each dataset is represented by a rectangular entity, and the relationships between entities are represented by diamonds. The ovals depict attributes belonging to each of the entities. Most of the relationships are one-to-many, meaning one entity connects to many other entities through the relationship (e.g., one ski resort contains many ski runs and ski lifts). The exception to this is the many-to-many relationships between trailheads, trail segments, and trail routes (e.g., several trailheads can initiate one route and several routes can begin at one trailhead).\nThe double lines connecting some entities to a relationship indicate that the specific entity is required to participate in the relationship. For example a ski lift must be within a ski resort, and a boat ramp must be adjacent to a river or lake. The third mandatory participation constraint is for the disjoint subtype of parks. This means that parks must either be local, state, or national parks, but they can only be in one of those three categories. Finally, among the attributes of each entity, the underlined attribute is the unique identifier for the entity that links allows it to be linked to other entities through the relationships.\n  Figure 2. Conceptual Model: Entity-Relationship Diagram   Logical Model The conceptual Entity-Relationship diagram was translated into a logical Relational Model diagram (Figure 3). In this process, primary and foreign keys were specified for each relation (entity) and many-to-many relationships were connected through the creation of an additional relation. Further, domains where added for each attribute, to include data type and length constraints. Three many-to-many relationships exist in the Utah Recreation Database, so relations were added for each. An “Initiation” relation was added between both Trailheads and Trail Routes and between Trailheads and Trail Segments. A “Connection” relation was also added between Trail Segments and Trail Routes. For the Boat Ramp relation, the “Water_body” attribute is used as the foreign key to both the Lake and River relations to connect Boat Ramps to appropriate water bodies. Finally, the Park supertype relation is linked to the “Local_Park,” “State_Park,” and “National_Park” subtype relations, where each park ID is both a foreign key and primary key.\n  Figure 3. Logical Model: Relational Model Diagram   Physical Model To translate the logical relational model into a physical model, a geodatabase was created and populated in ArcCatalog (Figure 4). A feature class was imported or created for each dataset and relationship classes were created to link feature classes together, where appropriate. Several of the relationship classes were simple, one-to-many relationships placing the feature within the state. However, a few required more careful design. The many-to-many relationships among trailheads, trail segments, and trail routes were created by selecting appropriate features and adding them to the associated relationship class’s table. Additionally, boat ramps were linked to their adjacent rivers and lakes through relationship classes and ski lifts and ski runs were connected to their parent ski resorts. In its completed form, the geodatabase provides a coherency to the data that enables additional functionality as described below.\nAfter putting the recreation database together, new functions are now available that weren’t possible when the datasets were disparate. This can be demonstrated by answering the questions in the introduction. In ArcMap, the trailheads leading to Goblin Valley’s “Dark Side of the Moon” trail can be identified by selecting the trail and finding its associated trailheads by clicking on the “TH_to_Routes” relationship class. By selecting “Chip’s Run” from the ski runs dataset and clicking on the “SkiResort_SkiRuns” relationship class, Snowbird is identified as the resort. Snowbird’s attributes note that it contains 11 ski lifts. Lastly, Lake Powell’s boat ramps can be found by selecting the lake and using the “Lakes_BoatRamps” relationship class. All of these functions, and many more, can now be executed quickly and easily to interrogate the database to meet a user’s needs.\n  Figure 4. Physical Model: ESRI File Geodatabase   Future Work While this database ended up relatively complete, there are several improvements that could be made in the future. Most of the datasets were gathered from open sources (listed in the report), resulting in inconsistent or minimal attributes. For example, it would be informative to have starting elevation, ending elevation, and vertical gain data for the ski runs, trail routes, etc. Additional attribute improvements or geometry changes would also lead to a better overall database (e.g., slot canyons represented as line segments). In some cases, the data were trimmed into subsets to make it more manageable (only ski runs in Snowbird were used, trail segments/routes were clipped to the San Rafael Swell/Goblin Valley region), so including more complete data would provide a better product. Finally, adding datasets for more recreation options would also produce a more comprehensive database (rock climbing locations, whitewater rafting/kayaking areas, etc.).\nA complete accounting of datasets, business rules, and references can be found in the Full Report.\nMSGIS Program Skills  Spatial Data and Algorithms Cartography and Graphic Design Data Models and Structures Database Design Structured Query Language (SQL) Project Management Communication Skills  ","date":1512345600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1512345600,"objectID":"462c8d649cc13da2fb6d5ac862c15db3","permalink":"https://eneemann.github.io/project/spatial-data-design/","publishdate":"2017-12-04T00:00:00Z","relpermalink":"/project/spatial-data-design/","section":"project","summary":"Recreation Database","tags":["Database Design","SQL","Data Models","Cartography","Project Management","Communication"],"title":"Spatial Database Design","type":"project"},{"authors":null,"categories":null,"content":" Slides Final Report  Background The goal of this course was to come up with an original project that could be used to demonstrate an end-to-end project management lifecycle, to include producing many of the project artifacts, without actually executing the project. In this case, producing a GIS database and mobile application for Salt Lake City’s Rose Park Golf Course was chosen to fulfill this goal. The project work emphasizes the Project Management Body of Knowledge’s ten knowledge areas, with specific focus on Scope, Time, Cost, Quality, Risk, and Procurement Management.\n  Schematic of Rose Park Golf Course GIS database   Introduction The Salt Lake City Municipal Golf Course system aims to provide an excellent recreation option and golfing experience to citizens, while maintaining a profitable or financially solvent enterprise. However, Salt Lake City’s municipal golf courses have been plagued by financial issues over the last several years due to a variety of factors, including: decreased golf rounds played, uncooperative weather, water costs, and land lease problems (Piper, 2017a). This has resulted in the closure of two golf courses (Jordan River Par 3 and Wingpointe; McKellar, 2015), and the transfer of Rose Park from the Golf Enterprise Fund to the General Fund of Salt Lake City’s budget (Mayor’s Proposed Budget, 2017). Rose Park was removed from the Golf Enterprise Fund because it operated at a net loss of $1.4 Million over the last 10-year period of complete data (2005-2014; Rowland, 2017). The Golf Enterprise Fund is intended to be a self-contained and self-sustaining city-run business venture where golf course profits are re-invested for course improvements. Just a few years ago it operated eight golf courses, but it now operates only five. Rose Park’s move to the General Fund by Mayor Biskupski essentially marks it as a taxpayer-funded course in order to preserve it as a desirable green space and community asset in the broader northwest Salt Lake City recreation area (Piper, 2017b). The move also protects the Golf Enterprise Fund from having to make up for Rose Park’s losses, increasing its own likelihood of success.\nRose Park’s financial struggles have presented Salt Lake City with an opportunity for it to return to profitability through the implementation of innovative solutions. The goal of this project is to implement a detailed GIS database and develop a mobile application to exploit the GIS data for financial benefit. The mobile app will have three primary functions:\n Golf Course Maintenance: This function will improve the efficiency of resource consumption for golf course and turf maintenance. It will include modules to estimate and predict the needs of water, seed, fertilizer, pesticide/herbicide, etc. Golf Course Management: This function will provide services for golfers, including tee time reservations, snack bar menu navigation, and food purchases. The course will also be able to manage and implement advertising within the application to promote local businesses and increase course revenue. Golfer GPS: This function will help improve the golf experience by exposing the GIS database for detailed distances to course features in a manner unlike any other golf apps on the market. It will help improve the golf experience and pace of play, leading to an increase in golf rounds played each year.    Figure 1. List of Project Management artifacts and deliverables   In turn, this project will help spur cost savings, increase the number of golfers at Rose Park, and increase profits by adding a new revenue stream through in-app advertising. If successful, this project will restore Rose Park’s financial stability, allowing it to return to the Golf Enterprise Fund with a positive impact. Additional details on the project, artifacts, and deliverables (Figure 2) can be found in the full project report and documentation. A few of these artifacts have been selected for discussion below.\n  Figure 2. Timeline for the Rose Park GIS Database \u0026amp; Mobile Application project   Scope Management The Rose park project will need to be completed on a precise timeline in order for the benefits to be realized at the start of the 2018 spring golf season. For this reason, scope management is critical to ensure that the work focuses on the agreed upon requirements and stakeholder needs. The documents below help define and control what work will be included in the project.\nBroad requirements outlining the scope of the project are listed below:\n Build comprehensive GIS database with features representing tees, greens, fairways, bunkers, water hazards, and other golf course infrastructure items. Provide enough attributes in the GIS database to improve the golf experience and course maintenance operations. Develop a mobile app with two primary interfaces:  Staff interface for course maintenance and golf management functions Public interface for golfers to access GPS distances and snack bar features   Mobile app capability for in-app advertising to increase revenue Mobile app capability for food/drink menu editing, viewing, and purchasing  Business Case Financials\nAs an attachment to the business case, the financials spreadsheet goes into detail on how the costs and benefits were estimated and how they are distributed over time, including discounted costs. This ultimately results in the calculation of ROI and NPV, as well as the payback period. Each of these elements are important in making a decision on whether or not to pursue a project. The calculation of numbers in the business case financials is further detailed in the cost estimate and benefit estimate spreadsheets attached to the final report.\n  Figure 3. Business case financial analysis   Time Management Along with scope management, time management is the other key aspect to ensure the Rose Park project is completed on time. The documents below were developed to provide tracking and a solid plan for the project schedule.\nGantt Chart\nA Gantt chart was created from the WBS described in the full report using MS Project and relied on the aforementioned dependencies and resources (shown on the chart) for auto-scheduling. It also displays summary tasks and identifies project milestones as red diamonds. The Gantt chart will be updated as the project progresses, using MS Project’s tracking functions.\n  Figure 4. Gantt chart showing projects tasks and timeline   Quality Management A high quality mobile app is needed to provide the services required by the management/maintenance staffs and to compete with commercial GPS golf apps. In order manage quality for the Rose Park project, stakeholder involvement will be relied on to provide quality assurance through multiple stakeholder acceptance instances and extensive user acceptance testing.\nQuality Management/Testing Process\nThe quality management and testing process flow chart describes the quality assurance/testing components in detail. Using Agile concepts, stakeholders will be involved from start to finish in the software development phase. This will begin with requirements definition and prototype acceptance and continue with several “gates” of stakeholder acceptance. Each sprint will have an iterative process of unit-level and integration testing, followed by bug fixes, and ending with a sprint demo and stakeholder acceptance. Software development will then be concluded with system-wide testing and stakeholder-driven user acceptance testing. The GIS database development is more straightforward and has a simpler quality management process. This will primarily consist of peer review among the GIS technicians, employing cause and effect diagrams when issues develop, and unit and integration testing for each database component.\n  Figure 5. Quality management and testing process for mobile application and GIS database   Risk Management With the previous financial struggles of Rose Park and thin margins in the overall golf course industry, project failure would be catastrophic for the course. To combat these perils, risk will be mitigated with the use of a risk register and probability/impact matrix, and updated frequently during bi-weekly risk review meetings.\nRisk Register\nThe risk register identifies several of the project’s biggest risks, along with a description, category, root cause, triggers, potential responses, and the risk owner for each. A probability and impact are also defined that, in conjunction with the risk matrix (not shown), help highlight risks that need to be monitored most closely. These risks are colored and ranked by their total risk score and ordered accordingly in the register. Those at the top pose the greatest threat. As situations change throughout the project, the risk register and statuses will be updated during the risk review meetings.\n  Figure 6. Risk register   Summary Recent struggles encountered by Salt Lake City golf courses have provided an opportunity for innovative solutions to improve profitability. The Rose Park GIS database and mobile application proposed here describe a path forward to return the city’s most troubled course to financial stability. This project has the support of the Mayor, an interested and enthusiastic set of stakeholders, a realistic plan, and resources available to execute the plan. The time has come to leverage data and technology to increase efficiency and profits for Rose Park.\nAll references and project artifacts can be found in the Full Report and attachments.\nMSGIS Program Skills  Cartography and Graphic Design Data Model and Structures Database Design Structured Query Language (SQL) Project Design Project Management Communication Skills  ","date":1511136000,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1511136000,"objectID":"9d9c34ceb8b0c1a50201ee433eeea314","permalink":"https://eneemann.github.io/project/project-management/","publishdate":"2017-11-20T00:00:00Z","relpermalink":"/project/project-management/","section":"project","summary":"Golf Course GIS Project","tags":["Database Design","SQL","Data Models","Cartography","Project Management","Project Design","Communication"],"title":"Project Management","type":"project"}]